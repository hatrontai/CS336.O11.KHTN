{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d3e8635",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-02-04T06:17:53.932983Z",
     "iopub.status.busy": "2024-02-04T06:17:53.932551Z",
     "iopub.status.idle": "2024-02-04T06:17:54.861260Z",
     "shell.execute_reply": "2024-02-04T06:17:54.859856Z"
    },
    "papermill": {
     "duration": 0.935911,
     "end_time": "2024-02-04T06:17:54.863359",
     "exception": false,
     "start_time": "2024-02-04T06:17:53.927448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from lavis.models import load_model_and_preprocess\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1e5e4532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tokenizer(): \n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    tokenizer.add_special_tokens({\"bos_token\": \"[DEC]\"})\n",
    "    tokenizer.add_special_tokens({\"additional_special_tokens\": [\"[ENC]\"]})\n",
    "    tokenizer.enc_token_id = tokenizer.additional_special_tokens_ids[0]\n",
    "    return tokenizer\n",
    "\n",
    "class blip2:\n",
    "    def __init__(self, model_size='pretrain'):\n",
    "        self.device = torch.device(\"cuda\")\n",
    "        self.model, self.vis_processor, self.text_processor = load_model_and_preprocess(name = \"blip2_feature_extractor\", \n",
    "                                                                          model_type = model_size, \n",
    "                                                                          is_eval = True, \n",
    "                                                                          device = self.device)\n",
    "        self.tokenizer = init_tokenizer()\n",
    "        self.model = self.model.to(torch.float)\n",
    "\n",
    "    def encode_image(self, image):\n",
    "        image_processed = self.vis_processor[\"eval\"](image).unsqueeze(0).to(torch.float).to(self.device)\n",
    "        \n",
    "        image_embeds = self.model.ln_vision(self.model.visual_encoder(image_processed))\n",
    "        image_embeds = image_embeds.float()\n",
    "        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(\n",
    "                    image_embeds.device\n",
    "                )\n",
    "        query_tokens = self.model.query_tokens.expand(image_embeds.shape[0], -1, -1)\n",
    "\n",
    "        query_output = self.model.Qformer.bert(\n",
    "                            query_embeds=query_tokens,\n",
    "                            encoder_hidden_states=image_embeds,\n",
    "                            encoder_attention_mask=image_atts,\n",
    "                            return_dict=True,\n",
    "                        )\n",
    "        image_feats = F.normalize(self.model.vision_proj(query_output.last_hidden_state), dim=-1)\n",
    "      \n",
    "        return image_feats[0][0].detach().cpu().numpy()\n",
    "\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        text_input = self.text_processor[\"eval\"](text)\n",
    "        text = self.tokenizer(text_input, return_tensors=\"pt\", padding=True).to(self.device)\n",
    "        text_output = self.model.Qformer.bert(\n",
    "                    text.input_ids,\n",
    "                    attention_mask=text.attention_mask,\n",
    "                    return_dict=True,\n",
    "                )\n",
    "        text_feat = F.normalize(\n",
    "                    self.model.text_proj(text_output.last_hidden_state[:, 0, :]), dim=-1\n",
    "                )\n",
    "        return text_feat[0].detach().cpu().numpy()\n",
    "    \n",
    "class blip:\n",
    "    def __init__(self, model_size = 'base', use_cpu = False): # model_size must be \"base\" or \"large\"\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if (use_cpu):\n",
    "            self.device = 'cpu'\n",
    "        \n",
    "        self.model, self.vis_processor, self.txt_processor = load_model_and_preprocess(name = \"blip_feature_extractor\", \n",
    "                                                                          model_type = model_size, \n",
    "                                                                          is_eval = True, \n",
    "                                                                          device = self.device)\n",
    "        self.tokenizer = init_tokenizer()\n",
    "        \n",
    "\n",
    "    def encode_image(self, image):\n",
    "        image_processed = self.vis_processor[\"eval\"](image).unsqueeze(0).to(self.device)\n",
    "        image_embeds = self.model.visual_encoder.forward_features(image_processed)\n",
    "        image_features = self.model.vision_proj(image_embeds)\n",
    "        image_features = F.normalize(image_features, dim=-1)\n",
    "      \n",
    "        embedding = image_features[0][0].detach().cpu().numpy() # get embedding of cls tokens on ViT for representation vector.\n",
    "        return embedding\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        text_input = self.txt_processor[\"eval\"](text)\n",
    "        text = self.tokenizer(text_input, return_tensors=\"pt\", padding=True).to(self.device)\n",
    "        text_output = self.model.text_encoder(\n",
    "                    text.input_ids,\n",
    "                    attention_mask = text.attention_mask,\n",
    "                    return_dict = True,\n",
    "                    mode = \"text\",\n",
    "                )\n",
    "        text_embeds = text_output.last_hidden_state\n",
    "        text_features = self.model.text_proj(text_embeds)\n",
    "        text_features = F.normalize(text_features, dim=-1)\n",
    "        embedding = text_features[0][0].detach().cpu().numpy() # get embedding of cls tokens on BERT for representation vector.\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "91761dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position interpolate from 16x16 to 26x26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.37G/4.37G [04:28<00:00, 17.5MB/s] \n"
     ]
    }
   ],
   "source": [
    "model = blip2('coco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fa570fbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-04T06:17:54.871161Z",
     "iopub.status.busy": "2024-02-04T06:17:54.869993Z",
     "iopub.status.idle": "2024-02-04T06:17:56.749389Z",
     "shell.execute_reply": "2024-02-04T06:17:56.747908Z"
    },
    "papermill": {
     "duration": 1.885529,
     "end_time": "2024-02-04T06:17:56.751861",
     "exception": false,
     "start_time": "2024-02-04T06:17:54.866332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "testset = pd.read_json('/home/nhan-softzone/cs336/CS336.O11.KHTN/data/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ac47a4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [02:50<00:00,  5.88it/s]\n"
     ]
    }
   ],
   "source": [
    "image_embeddings = []\n",
    "for i in tqdm(range(len(testset))):\n",
    "    image_embeddings.append(model.encode_image(Image.open('/home/nhan-softzone/cs336/flickr30k/' + testset.loc[i]['image']).convert('RGB')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7d53764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for i in range(len(testset)):\n",
    "    texts += testset.loc[i]['caption']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3ad170db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:28<00:00, 56.80it/s]\n"
     ]
    }
   ],
   "source": [
    "text_embeddings = []\n",
    "for i in tqdm(range(len(texts))):\n",
    "    text_embeddings.append(model.encode_text(texts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3a422a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = []\n",
    "for i in range(len(testset)):\n",
    "    for j in range(5):\n",
    "        ground_truth.append({'image_name' : testset.loc[i]['image'], ' comment' : testset.loc[i]['caption'][j]})\n",
    "ground_truth = pd.DataFrame(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "01e90149",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-04T06:17:58.963451Z",
     "iopub.status.busy": "2024-02-04T06:17:58.962553Z",
     "iopub.status.idle": "2024-02-04T06:17:58.971851Z",
     "shell.execute_reply": "2024-02-04T06:17:58.970843Z"
    },
    "papermill": {
     "duration": 0.023411,
     "end_time": "2024-02-04T06:17:58.978650",
     "exception": false,
     "start_time": "2024-02-04T06:17:58.955239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def recall_at_k(similarities, k):\n",
    "    hit_count = 0\n",
    "    for i in range(len(texts)):\n",
    "        text_caption = texts[i]\n",
    "        correct_image_name = caption_to_image[text_caption]\n",
    "        correct_index = image_names.index(correct_image_name)\n",
    "        \n",
    "        # Lấy k indices có giá trị similarity cao nhất\n",
    "        top_k_indices = np.argsort(-similarities[i])[:k]\n",
    "        \n",
    "        if correct_index in top_k_indices:\n",
    "            hit_count += 1\n",
    "    \n",
    "    return hit_count / len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "be9d1aa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-04T06:17:56.758821Z",
     "iopub.status.busy": "2024-02-04T06:17:56.758476Z",
     "iopub.status.idle": "2024-02-04T06:17:57.142120Z",
     "shell.execute_reply": "2024-02-04T06:17:57.141446Z"
    },
    "papermill": {
     "duration": 0.389574,
     "end_time": "2024-02-04T06:17:57.144145",
     "exception": false,
     "start_time": "2024-02-04T06:17:56.754571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5791950225830078\n",
      "Recall@1: 0.745\n",
      "Recall@5: 0.9378\n",
      "Recall@10: 0.9724\n",
      "Recall@20: 0.9868\n",
      "Recall@40: 0.994\n",
      "Recall@80: 0.998\n",
      "Recall@100: 0.998\n"
     ]
    }
   ],
   "source": [
    "# Chuyển đổi dữ liệu sang numpy arrays cho tính toán hiệu quả\n",
    "import time \n",
    "st_time = time.time()\n",
    "text_embeddings = np.array(text_embeddings)\n",
    "image_embeddings = np.array(image_embeddings)\n",
    "image_names = testset['image'].to_list()\n",
    "\n",
    "caption_to_image = {}\n",
    "for i in range(len(ground_truth)):\n",
    "    caption_to_image[ground_truth.iloc[i][' comment']] = ground_truth.iloc[i]['image_name']\n",
    "\n",
    "# Tính cosine similarity giữa text và image embeddings\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarities = text_embeddings @ image_embeddings.T\n",
    "\n",
    "# Tính Recall@1, Recall@5 và Recall@10\n",
    "recall_1 = recall_at_k(similarities, 1)\n",
    "recall_5 = recall_at_k(similarities, 5)\n",
    "recall_10 = recall_at_k(similarities, 10)\n",
    "print(time.time() - st_time)\n",
    "recall_20 = recall_at_k(similarities, 20)\n",
    "recall_40 = recall_at_k(similarities, 40)\n",
    "recall_80 = recall_at_k(similarities, 80)\n",
    "recall_100 = recall_at_k(similarities, 100)\n",
    "\n",
    "print(f\"Recall@1: {recall_1}\")\n",
    "print(f\"Recall@5: {recall_5}\")\n",
    "print(f\"Recall@10: {recall_10}\")\n",
    "print(f\"Recall@20: {recall_20}\")\n",
    "print(f\"Recall@40: {recall_40}\")\n",
    "print(f\"Recall@80: {recall_80}\")\n",
    "print(f\"Recall@100: {recall_100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8c322759",
   "metadata": {
    "papermill": {
     "duration": 0.002541,
     "end_time": "2024-02-04T06:18:19.937649",
     "exception": false,
     "start_time": "2024-02-04T06:18:19.935108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLIP2 Features Extractor - pretrain ViT-L\n",
      "Recall@1: 0.5264\n",
      "Recall@5: 0.7682\n",
      "Recall@10: 0.8444\n",
      "Recall@20: 0.9014\n",
      "Recall@40: 0.939\n",
      "Recall@80: 0.9706\n",
      "Recall@100: 0.979\n",
      "      \n",
      "BLIP2 Features Extractor - pretrain ViT-G\n",
      "Recall@1: 0.5052\n",
      "Recall@5: 0.7774\n",
      "Recall@10: 0.8552\n",
      "Recall@20: 0.9134\n",
      "Recall@40: 0.9536\n",
      "Recall@80: 0.9808\n",
      "Recall@100: 0.9862\n",
      "      \n",
      "BLIP Features Extractor - base\n",
      "Recall@1: 0.713\n",
      "Recall@5: 0.915\n",
      "Recall@10: 0.9494\n",
      "Recall@20: 0.9712\n",
      "Recall@40: 0.9856\n",
      "Recall@80: 0.9924\n",
      "Recall@100: 0.9942\n",
      "      \n",
      "BLIP2 COCO\n",
      "Recall@1: 0.745\n",
      "Recall@5: 0.9378\n",
      "Recall@10: 0.9724\n",
      "Recall@20: 0.9868\n",
      "Recall@40: 0.994\n",
      "Recall@80: 0.998\n",
      "Recall@100: 0.998\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "BLIP2 Features Extractor - pretrain ViT-L\n",
    "Recall@1: 0.5264\n",
    "Recall@5: 0.7682\n",
    "Recall@10: 0.8444\n",
    "Recall@20: 0.9014\n",
    "Recall@40: 0.939\n",
    "Recall@80: 0.9706\n",
    "Recall@100: 0.979\n",
    "      \n",
    "BLIP2 Features Extractor - pretrain ViT-G\n",
    "Recall@1: 0.5052\n",
    "Recall@5: 0.7774\n",
    "Recall@10: 0.8552\n",
    "Recall@20: 0.9134\n",
    "Recall@40: 0.9536\n",
    "Recall@80: 0.9808\n",
    "Recall@100: 0.9862\n",
    "      \n",
    "BLIP Features Extractor - base\n",
    "Recall@1: 0.713\n",
    "Recall@5: 0.915\n",
    "Recall@10: 0.9494\n",
    "Recall@20: 0.9712\n",
    "Recall@40: 0.9856\n",
    "Recall@80: 0.9924\n",
    "Recall@100: 0.9942\n",
    "      \n",
    "BLIP2 COCO\n",
    "Recall@1: 0.745\n",
    "Recall@5: 0.9378\n",
    "Recall@10: 0.9724\n",
    "Recall@20: 0.9868\n",
    "Recall@40: 0.994\n",
    "Recall@80: 0.998\n",
    "Recall@100: 0.998''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fc5ba137",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrievals = []\n",
    "ground_truths = []\n",
    "for i in range(len(image_names[:4])):\n",
    "    retrievals.append(list(np.argsort(-similarities.T[i])[:10]))\n",
    "    ground_truths.append([i * 5, i * 5 + 1, i * 5 + 2, i * 5 + 3, i * 5 + 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c9b04179",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_k(retrieved_items, ground_truth_items, k):\n",
    "    hits = sum(item in retrieved_items[:k] for item in ground_truth_items)\n",
    "    return hits / min(k, len(ground_truth_items))\n",
    "def overall_recall_at_k(all_retrievals, all_ground_truths, k):\n",
    "    recalls = [recall_at_k(all_retrievals[i], ground_truths[i], k) \n",
    "               for i in range(len(all_retrievals))]\n",
    "    return sum(recalls) / len(recalls) if recalls else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "77a51691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_recall_at_k(retrievals, ground_truths, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10c146c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4393313,
     "sourceId": 7549181,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 29.63102,
   "end_time": "2024-02-04T06:18:20.662969",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-04T06:17:51.031949",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
